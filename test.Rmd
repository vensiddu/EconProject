---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

The librarys that we are loading in.

```{r}
Packages <- c("ggplot2", "dplyr", "stargazer", "randomForest", "nnet", "caret")
lapply(Packages, library, character.only = TRUE)

```
Data import
```{r}
wind_turbine_data <- read.csv("uswtdb_v6_0_20230531.csv")
wind_ordinance_data <- read.csv("wind_ordinance_main.csv")
wind_resource_data <- read.csv("wtk_site_metadata.csv")
states <- read.csv("states.csv")
county_complete <- read.csv("county_complete.csv")
```
clean and group wind_resource_data
```{r}
wind_resource_data <- wind_resource_data[wind_resource_data$County != "Unknown", ]
wind_resource_data <- wind_resource_data[wind_resource_data$State != "Unknown", ]
wind_resource_data <- wind_resource_data %>%
  group_by(State, County) %>%
  summarise(across(c(fraction_of_usable_area, capacity, wind_speed, capacity_factor), mean), .groups = 'drop')
```
Clean wind turbine data
```{r}
wind_turbine_main <- wind_turbine_data[complete.cases(wind_turbine_data[, c("eia_id", "p_year", "p_cap", "t_manu", "t_model", "t_cap", "t_hh", "t_rd", "xlong", "ylat")]),]
wind_turbine_main <- wind_turbine_main[which(wind_turbine_main$p_year >= 2001),]
wind_turbine_main <- wind_turbine_main[which(!wind_turbine_main$t_state %in% c("AK", "HI")),]
wind_turbine_main$t_cap <- wind_turbine_main$t_cap/1000
wind_turbine_main$t_county <- gsub(' County', "", wind_turbine_main$t_county)
```
This is how to get the mode in factoral data
```{r}
calculate_mode <- function(x) {
  uniq_x <- unique(x)
  uniq_x[which.max(tabulate(match(x, uniq_x)))]
}
```
Data inspection 
```{r}
wind_project_data <- wind_turbine_main %>% dplyr::group_by(eia_id) %>% dplyr::summarise(p_cap_avg = mean(p_cap), t_cap_avg = mean(t_cap), operating_year = first(p_year), hub_ht = mean(t_hh), rotor_diam = mean(t_rd),t_manu = calculate_mode(t_manu), turbines = n(), t_model = calculate_mode(t_model),state = first(t_state), county = first(t_county), t_rsa = mean(t_rsa), t_ttlh  = mean(t_ttlh))
head(wind_project_data)
```
Wind ordinance data cleaning
```{r}
wind_ordinance_data$X <- NULL
wind_ordinance_main <- wind_ordinance_data[which(wind_ordinance_data$ordinance_year >= 2001),]
ordinance_State <- wind_ordinance_main %>% dplyr::group_by(State) %>% dplyr::summarise(tot_ord = n())
wind_ordinance_main$ordinance <- 1 
wind_resource_data <- wind_resource_data[which(!wind_resource_data$State %in% c("Alaska", "Hawaii")),]
```
County Complete geting Aera
```{r}
Area <- select(county_complete, state, name, area_2010)
Area$name <- gsub(' County', "", Area$name)
```
 Merging the datasets
```{r}
wind_ordinance_main <- merge(wind_ordinance_main, states, by.x = c("State"), by.y = c("State"), all.x = T)
Area_Final <- merge(Area, states, by.x = "state", by.y = "State", all.x = T)
wind_resource_Final <- merge(wind_resource_data, states, by.x = c("State"), by.y = c("State"), all.x = T)
turbine_ordinance_merge <- merge(wind_turbine_main, wind_ordinance_main, by.x = c("t_state", "t_county"), by.y = c("Abbreviation", "County"), all.x=T)
```
 More merging
```{r}
turbine_ordinance_merge$ordinance[is.na(turbine_ordinance_merge$ordinance)] <- 0
turbine_ordinance_merge$State <- NULL

turbine_ordinance_merge <- subset(turbine_ordinance_merge, select = -c(ordinance_year))
turbine_ordinance_merge <- subset(turbine_ordinance_merge, select = -c(case_id,faa_ors,faa_asn))
turbine_ordinance_merge <- subset(turbine_ordinance_merge, select = -c(usgs_pr_id,eia_id, t_fips))
turbine_ordinance_merge <- subset(turbine_ordinance_merge, select = -c(p_name,retrofit, retrofit_year,t_img_date,t_img_srce,xlong,ylat))
```
testing
```{r}
head(turbine_ordinance_merge)
```
group By State
```{r}
turbine_ordinance_county <- turbine_ordinance_merge %>% dplyr::group_by(t_state, t_county) %>% dplyr::summarise(tot_cap = sum(t_cap), avg_p_cap = mean(p_cap),avg_hh = mean(t_hh),avg_rd = mean(t_rd),ordinance = mean(ordinance),t_manu = calculate_mode(t_manu), turbines = n(),
t_model = calculate_mode(t_model),t_rsa = mean(t_rsa),t_ttlh  = mean(t_ttlh),
avg_t_cap = mean(t_cap))
```
testing
```{r}
head(turbine_ordinance_county)
```
More merging
```{r}
# Remove the "State" column from wind_resource_Final
wind_resource_Final <- wind_resource_Final[, !(names(wind_resource_Final) %in% "State")]

# Merge the datasets using Abbreviation and t_state
merged_data <- merge(turbine_ordinance_county, wind_resource_Final, by.x = c("t_state", "t_county"), by.y = c("Abbreviation", "County"))

Area_Final <- Area_Final[, !(names(Area_Final) %in% "state")]

merged_data <- merge(merged_data, Area_Final, by.x = c("t_state", "t_county"), by.y = c("Abbreviation", "name"))

```

Random Forest-data split
```{r}
# Split the dataset into training and testing sets
set.seed(123)  # for reproducibility
train_indices <- sample(1:nrow(merged_data), 0.7 * nrow(merged_data))  # 70% for training
train_data <- merged_data[train_indices, ]
test_data <- merged_data[-train_indices, ]
```
Random Forest-Model
```{r}
rf_model <- randomForest(tot_cap ~ ., data = train_data)
```
Random Forest-results
```{r}
predictions <- predict(rf_model, newdata = test_data)
mse <- mean((predictions - test_data$tot_cap)^2)
print(paste("Mean Squared Error (MSE):", mse))
```
RandOm Forest-Error-Per_perdiction 
```{r}
options(scipen = 999)
se = (predictions - test_data$tot_cap)^2
prediction_table <- data.frame(Real =test_data$tot_cap, Predicted = predictions, SE = se)
print(prediction_table)
```

```{r}
# Calculate mean
mean_value <- mean(test_data$tot_cap)
print(paste("Mean:", mean_value))

# Calculate standard deviation
sd_value <- sd(test_data$tot_cap)
print(paste("Standard Deviation:", sd_value))
```


More Random Forest
```{r}
# Set up cross-validation parameters
num_folds <- 5
control <- trainControl(method = "cv", number = num_folds)

# Initialize variables to store the best model and its MSE
best_model <- NULL
best_mse <- Inf

# Define the number of iterations for training
num_iterations <- 50

# Define other parameters for the random forest
# Number of trees in the forest
ntree <- 1000
# Number of variables to sample for each split
#mtry <- sqrt(ncol(train_data) - 1) + 2
mtry <- ncol(train_data) - 5

```
Loop
```{r}
for (i in 1:num_iterations) {
  # Train the random forest model
  rf_model <- randomForest(tot_cap ~ ., data = train_data, ntree = ntree, mtry = mtry)
  
  # Make predictions on the testing set
  predictions <- predict(rf_model, newdata = test_data)
  
  # Calculate Mean Squared Error (MSE)
  mse <- mean((predictions - test_data$tot_cap)^2)
  
  # Update best model if current model has lower MSE
  if (mse < best_mse) {
    best_model <- rf_model
    best_mse <- mse
  }
  
  # Print MSE for each iteration
  cat("Iteration", i, "- MSE:", mse, "\n")
}
```
Results
```{r}
# Print the final best MSE and best model details
cat("Best MSE:", best_mse, "\n")
print(best_model)
```

More Random Forest- Cross-Validation
```{r}
# Set seed for reproducibility
set.seed(123)

# Split data into training and testing datasets
index <- createDataPartition(y = merged_data$tot_cap, p = 0.8, list = FALSE)
training_data <- merged_data[index, ]
test_data <- merged_data[-index, ]

# Initialize variables to store results
best_model <- NULL
best_mse <- Inf
```
Loop-- Cross-Validation
```{r}
# Run the model 50 times
for (i in 1:50) {
  # Prepare the data
  predictors <- training_data[, !names(training_data) %in% c("tot_cap")]
  response <- training_data$tot_cap

  # Set up cross-validation
  ctrl <- trainControl(method = "cv",    # Cross-validation method
                       number = 5,       # Number of folds
                       repeats = 3)      # Number of repetitions

  # Train the random forest model
  model <- train(x = predictors,       # Predictor variables
                 y = response,         # Response variable
                 method = "rf",        # Random forest method
                 trControl = ctrl)     # Cross-validation control

  # Make predictions on the test dataset
  predictions <- predict(model, newdata = test_data)

  # Calculate Mean Squared Error (MSE)
  mse <- mean((predictions - test_data$tot_cap)^2)

  # Check if this model is better than the previous best model
  if (mse < best_mse) {
    best_model <- model
    best_mse <- mse
  }
  # Print MSE for each iteration
  cat("Iteration", i, "- MSE:", mse, "\n")
}

```
Results-- Cross-Validation
```{r}
# Print the best model
print(best_model)

# Print the best MSE
print(paste("Best Mean Squared Error:", best_mse))
```



Neural Network
```{r}

# Number of times to run the random forest algorithm
num_iterations <- 50

# Initialize variables to store best model information
best_model <- NULL
best_mse <- Inf
```
Loop
```{r}
for (i in 1:num_iterations) {
  # Train the neural network model
  nn_model <- nnet(tot_cap ~ ., data = train_data, size = 3, decay = 1e-5, maxit = 1000)
  
  # Make predictions on the testing set
  predictions <- predict(nn_model, newdata = test_data)
  
  # Calculate Mean Squared Error (MSE)
  mse <- mean((predictions - test_data$tot_cap)^2)
  print(mse)
  # Update best model if current model has lower MSE
  if (mse < best_mse) {
    best_model <- nn_model
    best_mse <- mse
  }
}
```
Results
```{r}
# Print the best model and its MSE
print("Best Model:")
print(best_model)
print(paste("Best MSE:", best_mse))
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

